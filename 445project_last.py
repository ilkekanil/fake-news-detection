# -*- coding: utf-8 -*-
"""445Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kx3LvxJeaqT4BQ6aVTAh37NyhstDhY4-

Fake News Detection

#1. Importing Libraries
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

import pandas as pd

# Load the dataset into a pandas DataFrame
data_fake = pd.read_csv('Fake.csv')
data_true = pd.read_csv('True.csv')

"""#2. Loading the Dataset"""

# Display the first few rows of each dataset
data_true.head()

data_fake.head()

# Labeling them seperately
data_fake['label'] = 0
data_true['label'] = 1

# Remove '(Reuters)' from the beginning of true news text
data_true['text'] = data_true['text'].str.replace(r'^\(Reuters\)\s*', '', regex=True)

# Merge and Remove Unnecessary Columns
df=pd.concat([data_true,data_fake],axis=0)

df.drop(['subject','date'],axis=1,inplace=True)

df.isnull().sum()

# Combine title and text into a single field
df['combined_text'] = df['title'] + " " + df['text']
df = df[['combined_text', 'label']]

print("Total rows:", len(df))
print(df["label"].value_counts())

df.head()

df.tail()

def clean_text(text):
    # Remove special characters, numbers, and extra spaces
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    text = text.lower()  # Convert to lowercase
    return text

df['cleaned_text'] = df['combined_text'].apply(clean_text)

df

# Split the dataset into training and testing sets
X = df['cleaned_text'] # Features
y = df['label']  # Target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""*Naive Bayes Classifier*"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    precision_recall_curve,
    auc
)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import ConfusionMatrixDisplay

class CustomNaiveBayes:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        self.model = MultinomialNB(alpha=self.alpha)

    def fit(self, X, y):
        self.model.fit(X, y)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def predict(self, X):
        return self.model.predict(X)

    def evaluate(self, X_test, y_test):
        # Predictions
        y_pred = self.predict(X_test)

        # 1) Print the Classification Report first
        print("Classification Report:")
        print(classification_report(
            y_test,
            y_pred,
            target_names=["Fake", "Real"],
            zero_division=0
        ))

        # 2) Print Macro Metrics
        macro_precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
        macro_recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
        macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)

        print("Macro Metrics:")
        print(f"Macro Precision: {macro_precision:.4f}")
        print(f"Macro Recall:    {macro_recall:.4f}")
        print(f"F1-Score:        {macro_f1:.4f}")

        # 3) Accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print(f"\nAccuracy:        {accuracy:.4f}")

        # 4) Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fake", "Real"])
        disp.plot(cmap='Blues')
        plt.title('Confusion Matrix')
        plt.show()

        # Return metrics
        return {
            'accuracy': accuracy,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1
        }

def train_fake_news_detector_naive_bayes(data):
    try:

        # TF-IDF vectorization
        vectorizer = TfidfVectorizer(
            max_features=800,
            stop_words='english',
            max_df=0.85,
            min_df=0.15
        )


        X = vectorizer.fit_transform(data['cleaned_text']).toarray()
        y = data['label'].values

        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=0.3,
            random_state=42,
            stratify=y
        )

        # Train the Naive Bayes model
        model = CustomNaiveBayes(alpha=1.0)
        model.fit(X_train, y_train)

        # Evaluate (prints out everything before the curve)
        results = model.evaluate(X_test, y_test)

        # 5) Precision-Recall Curve
        y_proba = model.predict_proba(X_test)[:, 1]
        precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_proba)
        pr_auc = auc(recall_vals, precision_vals)

        plt.figure(figsize=(10, 6))
        plt.plot(recall_vals, precision_vals, label=f'PR AUC = {pr_auc:.2f}', color='blue')
        plt.title('Precision-Recall Curve')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.legend()
        plt.grid()
        plt.show()

        return model, vectorizer, results

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None, None, None

def predict_news_article_naive_bayes(text, model, vectorizer):
    try:
        X = vectorizer.transform([text]).toarray()
        pred = model.predict(X)[0]
        prob = model.predict_proba(X)[0]

        print("\nPrediction Results:")
        print("-" * 50)
        print(f"Classification: {'Real' if pred == 1 else 'Fake'} News")
        print(f"Confidence: {max(prob):.2%}")

        return pred, prob

    except Exception as e:
        print(f"An error occurred during prediction: {str(e)}")
        return None, None


if __name__ == "__main__":
    model, vectorizer, results = train_fake_news_detector_naive_bayes(df)

    if model is not None:
        sample_text = "Sample news article text here."
        prediction, probability = predict_news_article_naive_bayes(sample_text, model, vectorizer)

"""*Logistic Regression*



"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix, classification_report, f1_score, precision_score, recall_score,
    precision_recall_curve, roc_curve, auc
)
import seaborn as sns

class CustomLogisticRegression:
    def __init__(self, learning_rate=0.6, iterations=100):
        self.lr = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None

    def sigmoid(self, z):
        z = np.clip(z, -500, 500)  # Prevent overflow
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_features = X.shape[1]
        self.weights = np.random.randn(n_features) * 0.3
        self.bias = np.random.randn() * 0.3
        m = len(y)

        for i in range(self.iterations):
            z = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(z)
            dw = (1/m) * np.dot(X.T, (predictions - y))
            db = (1/m) * np.sum(predictions - y)
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict_proba(self, X):
        return self.sigmoid(np.dot(X, self.weights) + self.bias)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

def train_fake_news_detector(data):
    # TF-IDF Vectorization
    vectorizer = TfidfVectorizer(
        max_features=800, stop_words='english', max_df=0.85, min_df=0.15
    )

    X = vectorizer.fit_transform(data['cleaned_text']).toarray()
    y = data['label'].values

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Model training
    model = CustomLogisticRegression()
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=["Fake", "Real"]))

    # Macro Metrics
    macro_precision = precision_score(y_test, y_pred, average='macro')
    macro_recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    print("\nMacro Metrics:")
    print(f"Macro Precision: {macro_precision:.4f}")
    print(f"Macro Recall:    {macro_recall:.4f}")
    print(f"F1-Score:        {f1:.4f}")

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    plt.plot(recall, precision, marker='.', label='Logistic Regression')
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend()
    plt.show()

    # Return results
    return model, vectorizer, {"macro_precision": macro_precision, "macro_recall": macro_recall, "f1_score": f1}

# Call the training function (assuming `data` is already prepared)
model, vectorizer, metrics = train_fake_news_detector(df)

# Print summary metrics
print("\nSummary Metrics:")
print(metrics)

"""*LSTM Model*"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, precision_recall_curve
)
from tensorflow.keras.callbacks import ModelCheckpoint

def lstm_model_pipeline(data, max_words=3000, max_len=100, embedding_dim=50, epochs=2, batch_size=256):

    # Tokenization and padding
    tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(data['cleaned_text'])
    X = tokenizer.texts_to_sequences(data['cleaned_text'])
    X = pad_sequences(X, maxlen=max_len, padding='post', truncating='post')
    y = data['label'].values

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Building the LSTM model with adjustments
    model = Sequential([
        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),
        LSTM(64, return_sequences=False),
        Dropout(0.8),
        Dense(16, activation='relu'),
        Dropout(0.6),
        Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Model Checkpoint Callback
    checkpoint = ModelCheckpoint('adjusted_lstm_model.keras', save_best_only=True, monitor='val_loss', mode='min')

    # Training the LSTM model
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[checkpoint])

    # Evaluating the model
    y_pred = (model.predict(X_test, verbose=0) > 0.5).astype("int32")
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

    # Precision-Recall Curve
    y_proba = model.predict(X_test, verbose=0)
    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_proba)
    plt.figure(figsize=(6, 4))
    plt.plot(recall_vals, precision_vals, marker='.', label='LSTM Model')
    plt.title("Precision-Recall Curve")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend()
    plt.grid()
    plt.show()

    # Return evaluation metrics and the trained model
    metrics = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1
    }
    return metrics, model


# Train the LSTM model
print("Training LSTM model...")
metrics, lstm_model = lstm_model_pipeline(df, epochs=2, batch_size=256)

# Print evaluation metrics in formatted style
print("\nModel Performance Metrics:")
print("-" * 70)
print(f"Accuracy:    {metrics['accuracy']:.4f}")
print(f"Precision:   {metrics['precision']:.4f}")
print(f"Recall:      {metrics['recall']:.4f}")
print(f"F1-Score:    {metrics['f1_score']:.4f}")
print("-" * 70)

"""*Random Forest*"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

class SimpleRandomForest:
    def __init__(self, n_trees=5, max_depth=2, min_samples_split=5, random_state=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []
        self.random_state = random_state

    def _bootstrap_sample(self, X, y):
        np.random.seed(self.random_state)
        n_samples = X.shape[0]
        indices = np.random.choice(n_samples, n_samples, replace=True)
        return X[indices], y[indices]

    def _most_common_label(self, y):
        return np.bincount(y).argmax()

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape

        # Stopping criteria
        if (depth >= self.max_depth) or (n_samples < self.min_samples_split) or (len(np.unique(y)) == 1):
            return self._most_common_label(y)

        # Randomly select features to split on
        feature_indices = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)

        # Find the best split
        best_split = {'info_gain': -1, 'feature_index': None, 'threshold': None, 'split': None}
        for feature_index in feature_indices:
            thresholds = np.unique(X[:, feature_index])
            for threshold in thresholds:
                split = self._split(X[:, feature_index], threshold)
                if not split['valid']:
                    continue
                info_gain = self._information_gain(y, split['left_indices'], split['right_indices'])
                if info_gain > best_split['info_gain']:
                    best_split = {
                        'info_gain': info_gain,
                        'feature_index': feature_index,
                        'threshold': threshold,
                        'split': (split['left_indices'], split['right_indices'])
                    }

        if best_split['info_gain'] == -1:
            return self._most_common_label(y)

        left_indices, right_indices = best_split['split']

        left_tree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)
        right_tree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)

        return {
            'feature_index': best_split['feature_index'],
            'threshold': best_split['threshold'],
            'left': left_tree,
            'right': right_tree
        }

    def _split(self, feature_column, threshold):
        left_indices = np.where(feature_column <= threshold)[0]
        right_indices = np.where(feature_column > threshold)[0]

        if len(left_indices) == 0 or len(right_indices) == 0:
            return {'valid': False}

        return {'valid': True, 'left_indices': left_indices, 'right_indices': right_indices}

    def _information_gain(self, y, left_indices, right_indices):
        p = len(left_indices) / len(y)
        return self._gini(y) - (p * self._gini(y[left_indices]) + (1 - p) * self._gini(y[right_indices]))

    def _gini(self, y):
        proportions = np.bincount(y) / len(y)
        return 1 - np.sum(proportions ** 2)

    def fit(self, X, y):
        np.random.seed(self.random_state)
        for _ in range(self.n_trees):
            X_sample, y_sample = self._bootstrap_sample(X, y)
            tree = self._grow_tree(X_sample, y_sample)
            self.trees.append(tree)

    def _predict_tree(self, x, tree):
        if not isinstance(tree, dict):
            return tree
        feature_index = tree['feature_index']
        threshold = tree['threshold']
        if x[feature_index] <= threshold:
            return self._predict_tree(x, tree['left'])
        else:
            return self._predict_tree(x, tree['right'])

    def predict(self, X):
        tree_predictions = np.array([self._predict_tree(x, tree) for tree in self.trees for x in X])
        # Majority vote
        return np.array([np.bincount(tree_predictions[:, i]).argmax() for i in range(len(X))])

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, PrecisionRecallDisplay
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# splitting to prevent data leakage problem and class imbalance problem
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
X_dev, X_test, y_dev, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)

# TF-IDF
vectorizer = TfidfVectorizer(max_features=85, stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_dev_tfidf = vectorizer.transform(X_dev)
X_test_tfidf = vectorizer.transform(X_test)

# Reseting indices for y_train
y_train = y_train.reset_index(drop=True)

#label noise
np.random.seed(42)
flip_indices = np.random.choice(len(y_train), size=int(0.02 * len(y_train)), replace=False)
y_train_noisy = y_train.copy()
y_train_noisy.iloc[flip_indices] = 1 - y_train_noisy.iloc[flip_indices]  # Flip labels

# Train the Random Forest model
rf_model = RandomForestClassifier(
    n_estimators=3,
    max_depth=1,
    max_features=0.2,
    random_state=42
)
rf_model.fit(X_train_tfidf, y_train_noisy)

# Evaluating the both development and test sets
y_dev_pred = rf_model.predict(X_dev_tfidf)
y_test_pred = rf_model.predict(X_test_tfidf)

# Combine predictions
y_combined = np.concatenate((y_dev, y_test))
y_combined_pred = np.concatenate((y_dev_pred, y_test_pred))

# classification report
combined_report = classification_report(y_combined, y_combined_pred)

# Calculation of accuracy
combined_accuracy = accuracy_score(y_combined, y_combined_pred)

# Print classification report
print("Unified Classification Report:")
print(combined_report)

# Metrics summary
metrics_summary = {
    "Combined Accuracy": combined_accuracy,
    "Macro Averages": {
        "Precision": classification_report(y_combined, y_combined_pred, output_dict=True)['macro avg']['precision'],
        "Recall": classification_report(y_combined, y_combined_pred, output_dict=True)['macro avg']['recall'],
        "F1-Score": classification_report(y_combined, y_combined_pred, output_dict=True)['macro avg']['f1-score']
    }
}

print("\nMetrics Summary:")
for key, value in metrics_summary.items():
    print(f"{key}: {value}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_combined, y_combined_pred)
plt.figure(figsize=(6, 6))
plt.imshow(conf_matrix, cmap="Blues")
plt.title("Confusion Matrix")
plt.colorbar()
plt.xticks([0, 1], labels=["Class 0", "Class 1"])
plt.yticks([0, 1], labels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        plt.text(j, i, conf_matrix[i, j], ha="center", va="center", color="black")
plt.show()

# Precision-Recall Curve
fig, ax = plt.subplots(figsize=(8, 6))
PrecisionRecallDisplay.from_predictions(y_combined, y_combined_pred, ax=ax)
ax.set_title("Precision-Recall Curve")
plt.show()

"""*CNN Model*"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier

class CNNFakeNewsDetector:
    def __init__(self, max_vocab_size=10000, max_sequence_length=300, embedding_dim=100): #initialization
        self.max_vocab_size = max_vocab_size
        self.max_sequence_length = max_sequence_length
        self.embedding_dim = embedding_dim
        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')
        self.model = None

    def preprocess_data(self, X_train, X_dev, X_test):
        # Tokenize and pad sequences
        self.tokenizer.fit_on_texts(X_train)
        X_train_seq = self.tokenizer.texts_to_sequences(X_train)
        X_dev_seq = self.tokenizer.texts_to_sequences(X_dev)
        X_test_seq = self.tokenizer.texts_to_sequences(X_test)

        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_sequence_length, padding='post', truncating='post')
        X_dev_pad = pad_sequences(X_dev_seq, maxlen=self.max_sequence_length, padding='post', truncating='post')
        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_sequence_length, padding='post', truncating='post')

        return X_train_pad, X_dev_pad, X_test_pad


    def build_model(self):
    # Define the CNN model
      self.model = Sequential([
        Embedding(input_dim=self.max_vocab_size, output_dim=20),
        Conv1D(filters=64, kernel_size=7, activation='relu'),  #number of filters
        GlobalMaxPooling1D(),
        Dropout(0.6),
        Dense(1, activation='sigmoid', kernel_regularizer='l2')  # L2 regularization
    ])
      self.model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])


    def train(self, X_train, y_train, X_dev, y_dev, epochs=5, batch_size=32):
        if self.model is None:
            raise ValueError("Model is not built. Call build_model() before training.")

        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_dev, y_dev),
            epochs=epochs,
            batch_size=batch_size,
            verbose=1
        )
        return history

    def evaluate(self, X_test, y_test):
        if self.model is None:
            raise ValueError("Model is not built. Call build_model() before evaluation.")

        y_test_pred = (self.model.predict(X_test) > 0.5).astype(int)
        accuracy = accuracy_score(y_test, y_test_pred)
        report = classification_report(y_test, y_test_pred)

        return accuracy, report

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import precision_recall_curve, auc

# splitting to prevent data leakage problem and also class imbalance problem
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
X_dev, X_test, y_dev, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)



cnn_detector = CNNFakeNewsDetector() #initialization
X_train_pad, X_dev_pad, X_test_pad = cnn_detector.preprocess_data(X_train, X_dev, X_test)

cnn_detector.build_model()
history = cnn_detector.train(X_train_pad, y_train, X_dev_pad, y_dev, epochs=4, batch_size=32)

# Evaluate the model and get predictions
accuracy, report = cnn_detector.evaluate(X_test_pad, y_test)

# Generate predictions for the test set
y_pred_prob = cnn_detector.model.predict(X_test_pad)  # Get probabilities
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary prediction numbers

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Calculate precision-recall curve
precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
average_precision = auc(recall, precision)

# Display precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f"AP = {average_precision:.2f}")
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="best")
plt.show()

# Print accuracy and classification report
print(f"Test Accuracy: {accuracy}")
print("\nClassification Report:\n", report)
